
## 군집분석           ##########################################################

# 01. 데이터 불러오기                  -----------------------------------------
data_df = read.csv("Utilities.csv",
                   header = TRUE,
                   na.strings = ".")
View(data_df)
str(data_df)
summary(data_df)



# 02. 데이터 전처리 하기               -----------------------------------------
# - 1. Compary -> row name으로 지정
row.names(data_df) = data_df[, 1]

data_df = data_df[, -1]   # compary 삭제
data_df


# - 2. 데이터 정규화(표준화)
data_df_norm = sapply(data_df, scale)

data_df_norm
    
    #               vars  n    mean      sd  median trimmed     mad     min      max    range  skew kurtosis     se
    # Fixed_charge     1 22    1.11    0.18    1.11    1.12    0.12    0.75     1.49     0.74 -0.02    -0.21   0.04
    # RoR              2 22   10.74    2.24   11.05   10.74    2.52    6.40    15.40     9.00 -0.06    -0.83   0.48
    # Cost             3 22  168.18   41.19  170.50  166.83   36.32   96.00   252.00   156.00  0.10    -0.56   8.78
    # Load_factor      4 22   56.98    4.46   56.35   56.82    5.11   49.80    67.60    17.80  0.41    -0.62   0.95
    # Demand_growth    5 22    3.24    3.12    3.00    3.19    2.67   -2.20     9.20    11.40  0.22    -0.69   0.66
    # Sales            6 22 8914.05 3549.98 8024.00 8571.61 2608.63 3300.00 17441.00 14141.00  0.82    -0.11 756.86
    # Nuclear          7 22   12.00   16.79    0.00    9.59    0.00    0.00    50.20    50.20  0.92    -0.72   3.58
    # Fuel_Cost        8 22    1.10    0.56    0.96    1.07    0.51    0.31     2.12     1.81  0.47    -1.25   0.12

    #               vars  n mean sd median trimmed  mad   min  max range  skew kurtosis   se
    # Fixed_charge     1 22    0  1  -0.02    0.01 0.64 -1.97 2.04  4.01 -0.02    -0.21 0.21
    # RoR              2 22    0  1   0.14    0.00 1.12 -1.93 2.08  4.01 -0.06    -0.83 0.21
    # Cost             3 22    0  1   0.06   -0.03 0.88 -1.75 2.03  3.79  0.10    -0.56 0.21
    # Load_factor      4 22    0  1  -0.14   -0.04 1.15 -1.61 2.38  3.99  0.41    -0.62 0.21
    # Demand_growth    5 22    0  1  -0.08   -0.02 0.86 -1.74 1.91  3.66  0.22    -0.69 0.21
    # Sales            6 22    0  1  -0.25   -0.10 0.73 -1.58 2.40  3.98  0.82    -0.11 0.21
    # Nuclear          7 22    0  1  -0.71   -0.14 0.00 -0.71 2.27  2.99  0.92    -0.72 0.21
    # Fuel_Cost        8 22    0  1  -0.26   -0.06 0.91 -1.43 1.82  3.25  0.47    -1.25 0.21

row.names(data_df_norm) = row.names(data_df)

data_df_norm


# 03. 기술통계 확인 하기               -----------------------------------------
library(psych)

describe(data_df_norm)

pairs.panels(data_df_norm)


# 04. 유클리드 거리 측정 하기          -----------------------------------------
# method = euclidean, maximum, manhattan, canberra, binary, minkowski

model_d = dist(data_df_norm, method = "euclidean")

model_d
#               Arizona   Boston  Central  Commonwealth       NY Florida  Hawaiian     Idaho Kentucky Madison    Nevada New England Northern Oklahoma Pacific     Puget San Diego Southern    Texas Wisconsin   United
# Boston       3.096154                                                                                                                                                                                              
# Central      3.679230 4.916465                                                                                                                                                                                     
# Commonwealth 2.462149 2.164213 4.107079                                                                                                                                                                            
# NY           4.123129 3.852850 4.468735     4.127368                                                                                                                                                               
# Florida      3.606269 4.218804 2.992760     3.201836 4.600183                                                                                                                                                      
# Hawaiian     3.901898 3.448346 4.217769     3.969367 4.596261 3.352919                                                                                                                                             
# Idaho        2.737407 3.892524 4.990876     3.692949 5.155516 4.913953  4.364509                                                                                                                                   
# Kentucky     3.253851 3.957125 2.752623     3.753627 4.489900 3.730814  2.796298 3.594824                                                                                                                          
# Madison      3.099116 2.705330 3.934935     1.491427 4.045276 3.829058  4.506512 3.673884 3.572023                                                                                                                 
# Nevada       3.491163 4.792640 5.902882     4.864730 6.460986 6.004557  5.995814 3.462587 5.175240 5.081469                                                                                                        
# New England  3.223138 2.432568 4.031434     3.498769 3.603863 3.738824  1.660047 4.059770 2.735861 3.942171 5.208504                                                                                               
# Northern     3.959637 3.434878 4.385973     2.577003 4.758059 4.554909  5.010221 4.140607 3.658647 1.407032 5.309741    4.496249                                                                                   
# Oklahoma     2.113490 4.323825 2.742000     3.230069 4.818803 3.469268  4.914949 4.335241 3.816443 3.610272 4.315584    4.335484 4.385649                                                                          
# Pacific      2.593481 2.501195 5.156977     3.190250 4.255251 4.065764  2.930142 3.849872 4.113606 4.264133 4.735659    2.328833 5.103646 4.239522                                                                 
# Puget        4.033051 4.837051 5.264442     4.967244 5.816715 5.842268  5.042444 2.201457 3.627307 4.531420 3.429962    4.617791 4.406173 5.169314 5.175157                                                        
# San Diego    4.396680 3.623588 6.356548     4.893679 5.628591 6.099456  4.577294 5.426511 4.901037 5.484537 4.751387    3.497555 5.606577 5.558002 3.399659 5.559320                                               
# Southern     1.877248 2.904409 2.723954     2.651532 4.338150 2.853942  2.949006 3.237409 2.428533 3.070750 3.945595    2.451935 3.780942 2.301050 2.998784 3.973815  4.426129                                     
# Texas        2.410434 4.634878 3.179392     3.464171 5.133791 2.581208  4.515428 4.107966 4.109049 4.130120 4.522319    4.414578 5.010864 1.876051 4.030721 5.232256  6.089597 2.473696                            
# Wisconsin    3.174488 2.997481 3.733274     1.816465 4.385852 2.912401  3.541931 4.094283 2.948021 2.054393 5.352136    3.430937 2.226493 3.744430 3.782111 4.823711  4.866540 2.922392 3.903723                   
# United       3.453407 2.318451 5.088018     3.884260 3.644137 4.628341  2.675404 3.977130 3.742680 4.361961 4.883977    1.384124 4.937119 4.926966 2.097150 4.568885  3.095002 3.185250 4.972551  4.145222         
# Virginia     2.509287 2.421916 4.109321     2.578463 3.771757 4.026935  4.000096 3.239374 3.208932 2.559945 3.436927    2.995066 2.739910 3.512207 3.352644 3.457129  3.628061 2.548060 3.967618  2.618050 3.012264


# 05. 계층적 군집화  하기          ---------------------------------------------
# method = single, complete, average, median, centroid, ward.D

# - 1. 단일연결법
model_s = hclust(model_d, method = "single")

plot(model_s, hang = -1, ann = FALSE)  # hang = -1 : 최상위 부터 출력

# - 2. 평균연결법
model_a = hclust(model_d, method = "average")

plot(model_a, hang = -1, ann = FALSE) 




# 06. 군집수 결정          -----------------------------------------------------
cutree(model_s, k = 6)

cutree(model_a, k = 6)

rbind(cutree(model_s, k = 6), cutree(model_a, k = 6))



# 07. Heatmap 그래프 그리기 ----------------------------------------------------
heatmap(as.matrix(data_df_norm),
        Colv = NA,
        hclustfun = hclust,
        col = rev(paste("gray", 1:99, sep="")))












